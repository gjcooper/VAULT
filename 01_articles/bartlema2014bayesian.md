---
cssclasses:
  - research_note
type: journalArticle
author: Bartlema, Annelies; Lee, Michael; Wetzels, Ruud; Vanpaemel, Wolf
title: "A Bayesian hierarchical mixture approach to individual differences: Case studies in selective attention and representation in category learning"
publication: Journal of Mathematical Psychology
date: 2014-04-01
citekey: bartlema2014bayesian
aliases:
  - "A Bayesian hierarchical mixture approach to individual differences: Case studies in selective attention and representation in category learning"
---

# A Bayesian hierarchical mixture approach to individual differences: Case studies in selective attention and representation in category learning

Bartlema, A., Lee, M., Wetzels, R., & Vanpaemel, W. (2014). A Bayesian hierarchical mixture approach to individual differences: Case studies in selective attention and representation in category learning. _Journal of Mathematical Psychology_, _59_, 132â€“150. [https://doi.org/10.1016/j.jmp.2013.12.002](https://doi.org/10.1016/j.jmp.2013.12.002)
[online](http://zotero.org/users/local/kZl3QdXV/items/K5S83NDL) [local](zotero://select/library/items/K5S83NDL) [pdf](file:///home/gjc216/Zotero/storage/PDL7XM4I/Bartlema%20et%20al.%20-%202014%20-%20A%20Bayesian%20hierarchical%20mixture%20approach%20to%20indivi.pdf)
 

 
%% begin notes %%

## My Thoughts

Here Annelies et al argue using the examples of [[W K Estes]] for an approach to handle the problem of aggregation over individuals not capturing the individual differences. A common sidestep of this is to model each individual separately, but then there is no constraint, no group effects in the modelling output.

Instead Annelies et al use Hierarchical Mixture Modelling, where a distinct set of separate models are part of a larger "mixture". These different models can be either different parameterisations, or completely different accounts of the data generating process. For example they contrast the [[Generalised Context Model|GCM]] with the [[Multiplicative Prototype Model]], a quite different account of how category learning occurs.

%% end notes %%

### Annotations

%% begin annotations %%

##### Imported on 2024-06-03 11:18 pm
>[!note|#ffd400]
> I like this figure showing the aggregate modelling, individual modelling, random effects (hierarchical) models, mixture models and finally the proposed hierarchical mixture models.

---
>[!quote|#ffd400]
>The bottom-left panel corresponds to the assumption that every individual is different, and that they are all independent of one another. There is now a single true point for each person, and no structure in the relationship between these points. This assumption of full individual differences corresponds to the case where a model is fit separately to each subject in an experiment. [(p. 133)](zotero://open-pdf/library/items/PDL7XM4I?page=133&annotation=363843W7)

---%% end annotations %%

## Item Notes

#### Tags

##### Keywords

#subject/model_selection #subject/parameter_estimation #subject/individual_differences #subject/bayesian_method #subject/category_learning #subject/hierarchical_mixture_model

##### Authors

[[Annelies Bartlema]] [[Michael Lee]] [[Ruud Wetzels]] [[Wolf Vanpaemel]]

##### Publication

#pub/journal_of_mathematical_psychology


%% Import Date: 2024-06-03T23:19:06.733+10:00 %%
